# -*- coding: utf-8 -*-
"""Графы.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15XZQzo-ZYygURc8WmN5ti7khqTkp_Mzw
"""

!pip install tensorflow==2.9.0
!pip install ampligraph

import numpy as np
import pandas as pd
import ampligraph
import tensorflow as tf

from rdflib import Graph, Namespace, URIRef, Literal, RDF

g = Graph()

g.parse('last.ttl', format="turtle")

RDF = Namespace("http://www.w3.org/1999/02/22-rdf-syntax-ns#")
RDFS = Namespace("http://www.w3.org/2000/01/rdf-schema#")
EX = Namespace("http://www.semanticweb.org/alina/ontologies/2024/9/untitled-ontology-13#")

query1 = """
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX ex: <http://www.semanticweb.org/alina/ontologies/2024/9/untitled-ontology-13#>
SELECT ?цель ?тренировки ?продолжительность ?частота_тренировки ?калорийность ?содержание_белка ?содержание_жиров ?содержание_углеводов ?заболевания
WHERE {
  ?цель ex:определяет_тренировки ?тренировки .
  ?тренировки ex:иметь_продолжительность ?продолжительность .
  ?цель ex:иметь_количество_тренировок ?частота_тренировки .
  ?цель ex:определяет_питание ?калорийность .
  ?калорийность ex:определяет_белки ?содержание_белка .
?калорийность ex:определяет_жиры ?содержание_жиров .
?калорийность ex:определяет_углеводы ?содержание_углеводов .
  ?тренировки ex:иметь_ограничения_по_заболеванию ?заболевания .
}
"""

results1 = g.query(query1)

with open('output.csv', 'w') as file:
    for row in results1:
        file.write(",".join(row) + "\n")

df = pd.read_csv("output.csv")

df.isna().sum()
df = df.dropna()

print(df)

df["train"] = (df.training == "верх_тела") | (df.training == "низ_тела") | (df.training == "растяжка") | (df.training == "силовая") | (df.training == "йога")

df.train.value_counts()

disease_dict = {
    'ConstraintАритмия': 0,
    'ConstraintАстма': 1,
    'ConstraintВоспалительные_Заболевания': 2,
    'ConstraintГломерулонефрит': 3,
    'ConstraintДиабет': 4,
    'ConstraintИшемия': 5,
    'ConstraintКрасная_Волчанка': 6,
    'ConstraintМигрень': 7,
    'ConstraintОстеоартрит': 8,
    'ConstraintПаркинсонизм': 9,
    'ConstraintПоликистоз': 10,
    'ConstraintРевматоидный_Артрит': 11,
    'ConstraintЭмфизема': 12,
    'ConstraintАддисонова_Болезнь': 13,
    'ConstraintГиперфункция_Щитовидной_Железы': 14,
    'ConstraintКардиомиопатия': 15,
    'ConstraintНевропатия': 16,
    'ConstraintОстеопороз': 17,
    'ConstraintПочечная_Недостаточность': 18,
    'ConstraintСклеродермия': 19,
    'ConstraintФибромиалгия': 20,
    'ConstraintЦелиакия': 21,
    'ConstraintНедостаточность_Щитовидной_Железы': 22,
    'ConstraintОнкология': 23,
    'ConstraintПосттравматический_Синдром': 24,
    'ConstraintТревожные_Расстройства': 25,
    'ConstraintЯзва': 26,
    'ConstraintГастрит': 27,
    'ConstraintНефрит': 28,
    'ConstraintПсориатический_Артрит': 29,
    'ConstraintСердечная_Недостаточность': 30,
    'ConstraintСиндром_Кушинга': 31,
    'ConstraintСколиоз': 32,
    'ConstraintГипертония': 33,
    'ConstraintРассеянный_Склероз': 34,
    'ConstraintСиндром_Раздраженного_Кишечника': 35,
    'ConstraintФиброз': 36,
    'ConstraintЭпилепсия': 37
}

# Создание уникальных идентификаторов для каждого субъекта и объекта
df["goal_id"] = "Goal" + df.goal.str.title().str.replace(" ", "")
df["training_id"] = "Training" + df.training.str.title().str.replace(" ", "")
df["duration_id"] = "Duration" + df.duration.str.title().str.replace(" ", "")
df["frequency_id"] = "Frequency" + df.frequency.str.title().str.replace(" ", "")
df["energy_id"] = "Energy" + df.energy.astype(str)
df["protein_id"] = "Protein" + df.protein.astype(str)
df["fats_id"] = "Fats" + df.fats.astype(str)
df["carbohydrates_id"] = "Carbohydrates" + df.carbohydrates.astype(str)
df["constraint_id"] = "Constraint" + df.constraint.str.title().str.replace(" ", "")
df["constraint_id"] = df["constraint_id"].map(disease_dict)


# Создание троек вида <subject, predicate, object>
triples = []

for _, row in df.iterrows():
    # Goal information
    goal = (row["goal_id"], "hasTraining", row["training_id"])
    duration = (row["goal_id"], "hasDuration", row["duration_id"])
    frequency = (row["goal_id"], "hasFrequency", row["frequency_id"])
    energy = (row["goal_id"], "hasEnergy", row["energy_id"])
    protein = (row["goal_id"], "hasProtein", row["protein_id"])
    fats = (row["goal_id"], "hasFats", row["fats_id"])
    carbohydrates = (row["goal_id"], "hasCarbohydrates", row["carbohydrates_id"])
    constraint = (row["goal_id"], "hasConstraint", row["constraint_id"])

    triples.extend((goal, duration, frequency, energy, protein, fats, carbohydrates, constraint))

print(triples)

# Преобразование троек в DataFrame
triples_df = pd.DataFrame(triples, columns=['subject', 'predicate', 'object'])
triples_df[(triples_df.subject=="GoalНабор_Мышечной_Массы") | (triples_df.object=="TrainingВерх_Тела")]

# Предположим, что у нас есть DataFrame `triples_df` с колонками 'subject', 'predicate', 'object'
# Преобразование DataFrame в массив NumPy
triples = triples_df.values

# Проверка, что все элементы в triples являются строками
assert all(isinstance(item, str) for row in triples for item in row), "All elements in triples must be strings"

from ampligraph.evaluation import train_test_split_no_unseen

X_train, X_valid = train_test_split_no_unseen(np.array(triples), test_size=255)

print('Train set size: ', X_train.shape)
print('Test set size: ', X_valid.shape)

from ampligraph.latent_features import ScoringBasedEmbeddingModel
from ampligraph.latent_features.loss_functions import get as get_loss
from ampligraph.latent_features.regularizers import get as get_regularizer

model = ScoringBasedEmbeddingModel(k=50,
                                   eta=5,
                                   scoring_type='ComplEx',
                                   seed=0)

# Optimizer, loss and regularizer definition
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
loss = get_loss('multiclass_nll')
regularizer = get_regularizer('LP', {'p': 3, 'lambda': 1e-5})

# Compilation of the model
model.compile(optimizer=optimizer, loss=loss, entity_relation_regularizer=regularizer)

model.fit(X_train,
          batch_size=int(X_train.shape[0] / 10),
          epochs=50, # Number of training epochs
          verbose=True # Displays a progress bar.
          )

ranks = model.evaluate(X_valid,
                      use_filter={'train': X_train,
                                  'test': X_valid},
                      corrupt_side='s,o',
                      verbose=True)

from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score

# Преобразование массива рангов в одномерный массив
flat_ranks = ranks.flatten()
# Фильтрация одномерного массива, чтобы оставить только положительные значения (начиная с 1)
res_ranks = flat_ranks[flat_ranks > 0]

mr = mr_score(res_ranks)
mrr = mrr_score(res_ranks)

print("MRR: %.2f" % (mrr))
print("MR: %.2f" % (mr))

hits_10 = hits_at_n_score(res_ranks, n=10)
print("Hits@10: %.2f" % (hits_10))
hits_3 = hits_at_n_score(res_ranks, n=3)
print("Hits@3: %.2f" % (hits_3))
hits_1 = hits_at_n_score(res_ranks, n=1)
print("Hits@1: %.2f" % (hits_1))

"""## **CLUSTERING**"""

!git clone https://github.com/wyldebeast-wunderliebe/incf.countryutils.git
!cd incf.countryutils && pip install .

!pip install adjustText

# Commented out IPython magic to ensure Python compatibility.
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from adjustText import adjust_text
from incf.countryutils import transformations
# %matplotlib inline

id_to_name_map = {**dict(zip(df.goal_id, df.goal)),
                  **dict(zip(df.training_id, df.training)),
                  **dict(zip(df.duration_id, df.duration)),
                  **dict(zip(df.frequency_id, df.frequency)),
                  **dict(zip(df.energy_id, df.energy)),
                  **dict(zip(df.protein_id, df.protein)),
                  **dict(zip(df.fats_id, df.fats)),
                  **dict(zip(df.carbohydrates_id, df.carbohydrates)),
                  **dict(zip(df.constraint_id, df.constraint))}
print(id_to_name_map)

# entities = pd.concat((df.goal_id[df["train"]],
#                       df.training_id[df["train"]],
#                       df.duration_id[df["train"]],
#                       df.frequency_id[df["train"]],
#                       df.energy_id[df["train"]],
#                       df.protein_id[df["train"]],
#                       df.fats_id[df["train"]],
#                       df.carbohydrates_id[df["train"]],
#                       df.constraint_id[df["train"]])).unique()
entities = df.constraint_id[df["train"]].unique()
entity_embeddings = dict(zip(entities, model.get_embeddings(entities)))

constraint_to_disease_type = {
  "красная_волчанка": "аутоимунные",
  "псориатический_артрит": "аутоимунные",
  "склеродермия": "аутоимунные",
    "астма": "дыхательные",
  "бронхит": "дыхательные",
  "фиброз": "дыхательные",
  "эмфизема": "дыхательные",
  "воспалительные_заболевания": "жкт",
  "астрит": "жкт",
  "синдром_раздраженного_кишечника": "жкт",
  "целиакия": "жкт",
  "язва": "жкт",
  "мигрень": "неврологические",
  "невропатия": "неврологические",
  "паркинсонизм": "неврологические",
  "рассеянный_склероз": "неврологические",
  "эпилепсия": "неврологические",
  "онкология": "онкологические",
  "остеоартрит": "опорно-двигательный_аппарат",
  "остеопороз": "опорно-двигательный_аппарат",
  "ревматоидный_артрит": "опорно-двигательный_аппарат",
  "сколиоз": "опорно-двигательный_аппарат",
  "фибромиалгия": "опорно-двигательный_аппарат",
  "гломерулонефрит": "почки",
  "нефрит": "почки",
  "поликистоз": "почки",
  "почечная_недостаточность": "почки",
  "анорексия_и_булимия": "психические",
  "биполярное_расстройство": "психические",
  "депрессия": "психические",
  "посттравматический_синдром": "психические",
  "тревожные_расстройства": "психические",
  "аритмия": "сердечно-сосудистые",
  "гипертония": "сердечно-сосудистые",
  "ишемия": "сердечно-сосудистые",
  "кардиомиопатия": "сердечно-сосудистые",
  "сердечная_недостаточность": "сердечно-сосудистые",
  "аддисонова_болезнь": "эндокринные",
  "гиперфункция_щитовидной_железы": "эндокринные",
  "диабет": "эндокринные",
  "нарушение_гормонального_фона": "эндокринные",
  "недостаточность_щитовидной_железы": "эндокринные",
  "синдром_Кушинга": "эндокринные"
}

constraints = df.constraint[df["train"]].unique()
print(constraints)

print(entities)


# # Get valid keys
# invalid_keys = model.get_invalid_keys(constraints)
# valid_constraints = [key for key in constraints if key not in invalid_keys]
# # valid_constraints = [constraint for constraint in constraints if constraint in valid_keys]

# # Generate embeddings for valid constraints
# embeddings = model.get_embeddings(valid_constraints)

# # Create a dictionary of valid constraint embeddings
# constraint_embeddings = dict(zip(valid_constraints, embeddings))

# # Output the resulting embeddings
# print(constraint_embeddings)

embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in entity_embeddings.values()]))

from ampligraph.discovery import find_clusters
from sklearn.cluster import KMeans

clustering_algorithm = KMeans(n_clusters=6, n_init=50, max_iter=500, random_state=0)
clusters = find_clusters(entities, model, clustering_algorithm, mode='e')

def plot_clusters(hue):
    np.random.seed(0)
    plt.figure(figsize=(12, 12))
    plt.title("{} embeddings".format(hue).capitalize())
    ax = sns.scatterplot(data=plot_df[plot_df.disease_type != "нет_заболеваний"],
                         x="embedding1", y="embedding2", hue=hue)
    texts = []
    for i, point in plot_df.iterrows():
        if np.random.random() < 0.1:  # Подписываем случайные точки
            texts.append(plt.text(point['embedding1'] + 0.02, point['embedding2'] + 0.01, str(point["disease_type"])))
    adjust_text(texts)

print(len(entities))
# print(plot_df)

plot_df = pd.DataFrame({
    "entities": entities,
    "embedding1": embeddings_2d[:, 0],
    "embedding2": embeddings_2d[:, 1],
    "disease_type": pd.Series(constraints).map(constraint_to_disease_type).fillna("нет_заболеваний"),
    "cluster": "cluster" + pd.Series(clusters).astype(str)
})

plot_clusters("disease_type")

plot_clusters("cluster")

from sklearn import metrics
metrics.adjusted_rand_score(plot_df.disease_type, plot_df.cluster)

"""## Classification"""

print(df.constraint[df["train"]].unique())
constraints
# count = 0
# for constraint in df.constraint:
#     print(constraint, end=" ")
#     count += (constraint == "lupus")
# count

df["results"] = (df.constraint == "язва").astype(int) + \
                (df.constraint != "язва").astype(int)*2 - 1

df.constraint.value_counts(normalize=True)

df.results.value_counts(normalize=True)

"""## Original Dataset"""

!pip install xgboost

new_df = df[["results", "constraint_id", "train"]].copy()
new_df

disease_dict = {
    'ConstraintАритмия': 0,
    'ConstraintАстма': 1,
    'ConstraintВоспалительные_Заболевания': 2,
    'ConstraintГломерулонефрит': 3,
    'ConstraintДиабет': 4,
    'ConstraintИшемия': 5,
    'ConstraintКрасная_Волчанка': 6,
    'ConstraintМигрень': 7,
    'ConstraintОстеоартрит': 8,
    'ConstraintПаркинсонизм': 9,
    'ConstraintПоликистоз': 10,
    'ConstraintРевматоидный_Артрит': 11,
    'ConstraintЭмфизема': 12,
    'ConstraintАддисонова_Болезнь': 13,
    'ConstraintГиперфункция_Щитовидной_Железы': 14,
    'ConstraintКардиомиопатия': 15,
    'ConstraintНевропатия': 16,
    'ConstraintОстеопороз': 17,
    'ConstraintПочечная_Недостаточность': 18,
    'ConstraintСклеродермия': 19,
    'ConstraintФибромиалгия': 20,
    'ConstraintЦелиакия': 21,
    'ConstraintНедостаточность_Щитовидной_Железы': 22,
    'ConstraintОнкология': 23,
    'ConstraintПосттравматический_Синдром': 24,
    'ConstraintТревожные_Расстройства': 25,
    'ConstraintЯзва': 26,
    'ConstraintГастрит': 27,
    'ConstraintНефрит': 28,
    'ConstraintПсориатический_Артрит': 29,
    'ConstraintСердечная_Недостаточность': 30,
    'ConstraintСиндром_Кушинга': 31,
    'ConstraintСколиоз': 32,
    'ConstraintГипертония': 33,
    'ConstraintРассеянный_Склероз': 34,
    'ConstraintСиндром_Раздраженного_Кишечника': 35,
    'ConstraintФиброз': 36,
    'ConstraintЭпилепсия': 37
}

encoded_cols = pd.get_dummies(df[["goal", "training", "duration", "frequency", "energy", "protein", "fats", "carbohydrates"]])
encoded_cols

new_df = new_df.join(encoded_cols)
new_df

from xgboost import XGBClassifier

clf_model = XGBClassifier(n_estimators=500, max_depth=5, objective="multi:softmax", num_class=3)

X_train = new_df[df["train"]].drop(["results"], axis=1)
y_train = new_df[df["train"]].results
X_val = new_df[~df["train"]].drop(["results"], axis=1)
y_val = new_df[~df["train"]].results

num_classes = 38
clf_model.fit(X_train, y_train, verbose=1)

from sklearn import metrics
metrics.accuracy_score(y_val, clf_model.predict(X_val))

"""### Graph embedings"""

# Функция для получения признаков и целевой переменной
def get_features_target(mask):
    def get_embeddings(team):
        return entity_embeddings.get(team, np.full(200, np.nan))

    X = np.hstack((np.vstack(df[mask].goal_id.apply(get_embeddings).values),
                   np.vstack(df[mask].training_id.apply(get_embeddings).values)))
    y = df[mask].results.values
    return X, y

clf_X_train, y_train = get_features_target((df["train"]))
clf_X_test, y_test = get_features_target((~df["train"]))

clf_X_train.shape, clf_X_test.shape

np.isnan(clf_X_test).sum()/clf_X_test.shape[1]

clf_model = XGBClassifier(n_estimators=500, max_depth=5, objective="multi:softmax", num_class=3)

clf_model.fit(clf_X_train, y_train)

df[~df["train"]].results.value_counts(normalize=True)

metrics.accuracy_score(y_test, clf_model.predict(clf_X_test))

"""## Link prediction"""

X_train, X_valid = train_test_split_no_unseen(np.array(triples), test_size=255)

df = pd.DataFrame(X_train,columns = ['subject','predicate','object'])
goalSubject = "GoalНабор_Мышечной_Массы"
print(df[df.subject==goalSubject])

dfFiltered = np.array(df[(df.subject!=goalSubject) | ((df.subject==goalSubject) & ~df.predicate.isin(["hasConstraint"]))])

model.fit(dfFiltered)

statements = np.array([
    [f'{goalSubject}', 'hasConstraint', '0.0'],
    [f'{goalSubject}', 'hasConstraint', '1.0'],
    [f'{goalSubject}', 'hasConstraint', '2.0'],
    [f'{goalSubject}', 'hasConstraint', '3.0'],
    [f'{goalSubject}', 'hasConstraint', '4.0'],
    [f'{goalSubject}', 'hasConstraint', '5.0'],
    [f'{goalSubject}', 'hasConstraint', '6.0'],
    [f'{goalSubject}', 'hasConstraint', '7.0'],
    [f'{goalSubject}', 'hasConstraint', '8.0'],
    [f'{goalSubject}', 'hasConstraint', '9.0'],
    [f'{goalSubject}', 'hasConstraint', '10.0'],
    [f'{goalSubject}', 'hasConstraint', '11.0'],
    [f'{goalSubject}', 'hasConstraint', '12.0'],
    [f'{goalSubject}', 'hasConstraint', '13.0'],
    [f'{goalSubject}', 'hasConstraint', '14.0'],
    [f'{goalSubject}', 'hasConstraint', '15.0'],
    [f'{goalSubject}', 'hasConstraint', '16.0'],
    [f'{goalSubject}', 'hasConstraint', '17.0'],
    [f'{goalSubject}', 'hasConstraint', '18.0'],
    [f'{goalSubject}', 'hasConstraint', '19.0'],
    [f'{goalSubject}', 'hasConstraint', '20.0'],
])

statements_filter = np.array(list({tuple(i) for i in np.vstack((dfFiltered, statements))}))
statements_filter

ranks = model.evaluate(statements,
                      use_filter={'train': dfFiltered,
                                  'test': statements},
                      corrupt_side='s,o',
                      verbose=True)

scores = model.predict(statements)
scores

from scipy.special import expit
probs = expit(scores)

pd.DataFrame(list(zip([' '.join(x) for x in statements],
                      ranks,
                      np.squeeze(scores),
                      np.squeeze(probs))),
             columns=['statement', 'rank', 'score', 'prob']).sort_values("prob")